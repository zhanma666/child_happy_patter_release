# Happy Partner Ollamaæœ¬åœ°æ¨¡å‹éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•åœ¨æœ¬åœ°éƒ¨ç½²å¾®è°ƒåçš„emotion_finalæ¨¡å‹ï¼Œå¹¶å°†å…¶é›†æˆåˆ°Happy Partnerç³»ç»Ÿä¸­ä½œä¸ºæ•™è‚²å’Œæƒ…æ„Ÿä»£ç†ï¼Œå……åˆ†å‘æŒ¥ç¡¬ä»¶æ€§èƒ½ï¼Œå°†APIè°ƒç”¨ä»äº‘æœåŠ¡åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹ã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©Linux + AMDç»„åˆï¼Ÿ

### ğŸ® AMDé”é¾™AI MAX+395ä¼˜åŠ¿
- **Ryzen AIå¼•æ“**: å†…ç½®ä¸“ç”¨AIåŠ é€Ÿç¡¬ä»¶
- **å¤šæ ¸æ€§èƒ½**: 16æ ¸å¿ƒ32çº¿ç¨‹ï¼Œé€‚åˆå¹¶è¡Œæ¨ç†
- **ROCmæ”¯æŒ**: AMDå¼€æºè®¡ç®—å¹³å°ï¼ŒGPUåŠ é€Ÿ
- **èƒ½æ•ˆæ¯”**: 7nmå·¥è‰ºï¼Œä½åŠŸè€—é«˜æ€§èƒ½

### ğŸ§ Linuxç³»ç»Ÿä¼˜åŠ¿
- **åŸç”Ÿæ”¯æŒ**: Ollamaå®˜æ–¹æ¨èLinuxç¯å¢ƒ
- **æ€§èƒ½ä¼˜åŒ–**: æ›´ä½çš„ç³»ç»Ÿå¼€é”€ï¼Œæ›´å¥½çš„ç¡¬ä»¶è°ƒåº¦
- **å·¥å…·é“¾**: å®Œå–„çš„AIå¼€å‘å’Œéƒ¨ç½²å·¥å…·
- **ç¨³å®šæ€§**: é•¿æ—¶é—´è¿è¡Œç¨³å®šå¯é 

## éƒ¨ç½²æ­¥éª¤

### 1. ç³»ç»Ÿå‡†å¤‡

#### æ¨èLinuxå‘è¡Œç‰ˆ
- **Ubuntu 22.04 LTS** (é¦–é€‰æ¨è)
- **Fedora 39** (æ¬¡é€‰)
- **Arch Linux** (é«˜çº§ç”¨æˆ·)

#### ç³»ç»Ÿè¦æ±‚
- CPU: AMDé”é¾™AI MAX+395
- å†…å­˜: 16GBä»¥ä¸Š (æ¨è32GB)
- å­˜å‚¨: 50GBä»¥ä¸Šå¯ç”¨ç©ºé—´
- ç½‘ç»œ: ç¨³å®šçš„äº’è”ç½‘è¿æ¥ (åˆå§‹æ¨¡å‹ä¸‹è½½)

#### ç³»ç»Ÿä¼˜åŒ–
```bash
# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…å¿…è¦å·¥å…·
sudo apt install -y curl wget git python3 python3-pip python3-venv htop

# åˆ›å»ºé¡¹ç›®ç”¨æˆ· (å¯é€‰)
sudo useradd -m -s /bin/bash happy
sudo usermod -aG sudo happy
```

### 2. å®‰è£…Ollama

#### å®˜æ–¹å®‰è£…
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# éªŒè¯å®‰è£…
ollama --version

# å¯åŠ¨æœåŠ¡
sudo systemctl start ollama
sudo systemctl enable ollama
```

#### æ€§èƒ½ä¼˜åŒ–
```bash
# è¿è¡ŒAMDæ€§èƒ½ä¼˜åŒ–è„šæœ¬
sudo chmod +x scripts/optimize_amd_performance.sh
sudo ./scripts/optimize_amd_performance.sh

# é‡å¯ç³»ç»Ÿä»¥åº”ç”¨ä¼˜åŒ–
sudo reboot
```

### 3. éƒ¨ç½²emotion_finalæ¨¡å‹

#### 3.1 å‡†å¤‡emotion_finalæ¨¡å‹
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la /home/datawhale/Projects/emotion_final/

# å¦‚æœæœ‰GGUFæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
# å¦‚æœæ²¡æœ‰ï¼Œéœ€è¦å…ˆè½¬æ¢æ¨¡å‹æ ¼å¼
```

#### 3.2 åˆ›å»ºemotion_finalæ¨¡å‹
```bash
# è¿›å…¥æ¨¡å‹ç›®å½•
cd /home/datawhale/Projects/emotion_final

# åˆ›å»ºModelfile
cat > Modelfile << 'EOF'
FROM ./emotion_final_q4_k_m.gguf
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 2048
PARAMETER num_predict 512
PARAMETER num_thread 16
PARAMETER num_batch 512
PARAMETER f16_kv true
PARAMETER use_mmap true
PARAMETER mlock true
EOF

# åˆ›å»ºOllamaæ¨¡å‹
ollama create emotion_final -f Modelfile
```

#### 3.3 éªŒè¯æ¨¡å‹éƒ¨ç½²
```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list

# æµ‹è¯•æ¨¡å‹è¿è¡Œ
ollama run emotion_final "ä½ å¥½ï¼Œæˆ‘æ„Ÿåˆ°å¾ˆéš¾è¿‡"

# æµ‹è¯•æ•™è‚²åŠŸèƒ½
ollama run emotion_final "1+1ç­‰äºå¤šå°‘ï¼Ÿ"

# æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
ollama show emotion_final
```

#### 3.4 å¯é€‰ï¼šå®‰è£…ROCmæ”¯æŒGPUåŠ é€Ÿ
```bash
# å®‰è£…ROCm (Ubuntu 22.04)
sudo apt install -y wget
wget https://repo.radeon.com/amdgpu-install/6.0.2/ubuntu/jammy/amdgpu-install_6.0.2.60002-1_all.deb
sudo apt install -y ./amdgpu-install_6.0.2.60002-1_all.deb
sudo amdgpu-install --usecase=rocm,hip,mllib --no-dkms

# éªŒè¯ROCmå®‰è£…
rocm-smi

# é‡å¯OllamaæœåŠ¡ä»¥åº”ç”¨GPUæ”¯æŒ
sudo systemctl restart ollama
```

### 4. é…ç½®Happy Partnerç³»ç»Ÿ

#### 4.1 é¡¹ç›®éƒ¨ç½²
```bash
# å…‹éš†é¡¹ç›® (å¦‚æœè¿˜æ²¡æœ‰)
cd /home/happy
git clone <your-repo-url>
cd child_happy_patter_release

# åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
cd backend
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp ../.env.example .env
```

#### 4.2 ç¯å¢ƒå˜é‡é…ç½®
ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š
```env
# å¯ç”¨Ollamaæœ¬åœ°æ¨¡å‹
USE_OLLAMA=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=emotion_lora
OLLAMA_TIMEOUT=60

# AMDä¼˜åŒ–é…ç½®
OLLAMA_NUM_THREAD=16
OLLAMA_NUM_BATCH=512
OLLAMA_MAX_LOADED_MODELS=4
OLLAMA_FLASH_ATTENTION=1

# OpenAIé…ç½®ï¼ˆå¤‡ç”¨ï¼‰
OPENAI_API_KEY=sk-U6KEVS6duof2hKtKwxuuX6NidqLOrlAq0REQpxoAVa1keego
OPENAI_BASE_URL=https://api2.aigcbest.top/v1
```

#### 4.3 éªŒè¯é…ç½®
é…ç½®æ–‡ä»¶å·²æ›´æ–°æ”¯æŒOllamaï¼š
- `backend/config/settings.py`: Ollamaé…ç½®
- `backend/utils/ollama_client.py`: Ollamaå®¢æˆ·ç«¯
- `backend/utils/openai_client.py`: ç»Ÿä¸€æ¥å£

### 5. è¿è¡Œæµ‹è¯•å’Œå¯åŠ¨æœåŠ¡

#### 5.1 è¿è¡Œé›†æˆæµ‹è¯•
```bash
cd /home/happy/child_happy_patter_release/backend
source ../venv/bin/activate
python test_ollama_integration.py
```

#### 5.2 æ€§èƒ½ç›‘æ§
```bash
# å¯åŠ¨æ€§èƒ½ç›‘æ§
sudo /usr/local/bin/monitor_amd_performance.sh

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯æŸ¥çœ‹ç³»ç»Ÿèµ„æº
htop
```

#### 5.3 å¯åŠ¨åç«¯æœåŠ¡
```bash
# æ–¹æ³•1ï¼šç›´æ¥è¿è¡Œ
cd /home/happy/child_happy_patter_release/backend
source ../venv/bin/activate
python main.py

# æ–¹æ³•2ï¼šä½¿ç”¨ä¸€é”®å¯åŠ¨è„šæœ¬
sudo /usr/local/bin/start_happy_partner.sh
```

#### 5.4 æµ‹è¯•APIæ¥å£
```bash
# æµ‹è¯•èŠå¤©æ¥å£
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "session_id": 1,
    "content": "æˆ‘æ„Ÿåˆ°å¾ˆéš¾è¿‡"
  }'

# æµ‹è¯•Emotion Agent
curl -X POST "http://localhost:8000/emotion/support" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 1,
    "content": "æˆ‘åœ¨å­¦æ ¡è¢«æ¬ºè´Ÿäº†",
    "emotion_type": "éš¾è¿‡"
  }'
```

#### 5.5 å‹åŠ›æµ‹è¯•
```bash
# å®‰å‹æµ‹å·¥å…·
pip install locust

# åˆ›å»ºå‹æµ‹è„šæœ¬ locustfile.py
cat > locustfile.py << 'EOF'
from locust import HttpUser, task, between
import json

class HappyPartnerUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def chat_request(self):
        headers = {"Content-Type": "application/json"}
        data = {
            "user_id": 1,
            "session_id": 1,
            "content": "ä½ å¥½ï¼Œæˆ‘æ„Ÿåˆ°å¾ˆå¼€å¿ƒ"
        }
        self.client.post("/chat", json=data, headers=headers)
    
    @task(3)
    def emotion_request(self):
        headers = {"Content-Type": "application/json"}
        data = {
            "user_id": 1,
            "content": "æˆ‘ä»Šå¤©å¾ˆéš¾è¿‡",
            "emotion_type": "éš¾è¿‡"
        }
        self.client.post("/emotion/support", json=data, headers=headers)
EOF

# è¿è¡Œå‹æµ‹
locust -f locustfile.py --host=http://localhost:8000
```

## æ¨¡å‹ç‰¹æ€§

### emotion_loraæ¨¡å‹ç‰¹ç‚¹
- **åŸºç¡€æ¨¡å‹**: qwen2.5:0.5b
- **LoRAå¾®è°ƒ**: ä¸“é—¨é’ˆå¯¹å„¿ç«¥æƒ…æ„Ÿé™ªä¼´ä¼˜åŒ–
- **è§’è‰²è®¾å®š**: å½©è™¹å°ç²¾çµï¼Œä¸“é—¨é™ªä¼´3-12å²å°æœ‹å‹
- **æ ¸å¿ƒåŠŸèƒ½**: æƒ…ç»ªè¯†åˆ«ã€æƒ…æ„Ÿæ”¯æŒã€æ¸¸æˆåŒ–äº’åŠ¨

### æ”¯æŒçš„æƒ…æ„Ÿç±»å‹
- ç”Ÿæ°” â†’ "å°ç«é¾™åœ¨å–·ç«å•¦ï¼"
- éš¾è¿‡ â†’ "å¿ƒé‡Œä¸‹é›¨äº†"
- å®³æ€• â†’ "å°å¿ƒè„åœ¨è¹¦è¹¦è·³"
- è®¨åŒ â†’ "å°çœ‰æ¯›çš±èµ·æ¥å•¦"
- å…¶ä»–å¤æ‚æƒ…ç»ª

### å›å¤ç‰¹ç‚¹
- ä½¿ç”¨çŸ­å¥ï¼ˆæ¯å¥ä¸è¶…è¿‡15ä¸ªå­—ï¼‰
- å¤šç”¨æ‹Ÿå£°è¯å’Œè¡¨æƒ…ç¬¦å·
- ç”¨å­©å­èƒ½ç†è§£çš„æ¯”å–»
- ç§¯ææ­£é¢ï¼Œé¿å…å¦å®šæ„Ÿå—
- é¼“åŠ±ä¸çˆ¶æ¯æ²Ÿé€š

## AMDé”é¾™AI MAX+395æ€§èƒ½ä¼˜åŒ–

### ğŸ® ç¡¬ä»¶æ€§èƒ½å‘æŒ¥

#### CPUä¼˜åŒ–
```bash
# æŸ¥çœ‹CPUä¿¡æ¯
lscpu | grep "Model name"
cat /proc/cpuinfo | grep "cpu cores"

# è®¾ç½®CPUæ€§èƒ½æ¨¡å¼
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# å¯ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
echo 1 | sudo tee /sys/devices/system/cpu/cpu*/online
```

#### å†…å­˜ä¼˜åŒ–
```bash
# æŸ¥çœ‹å†…å­˜ä¿¡æ¯
free -h
cat /proc/meminfo

# ç¦ç”¨é€æ˜å¤§é¡µ (æå‡AIæ€§èƒ½)
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled

# è°ƒæ•´å†…å­˜è¿‡commitç­–ç•¥
echo 1 | sudo tee /proc/sys/vm/overcommit_memory
```

#### GPUåŠ é€Ÿ (å¯é€‰)
```bash
# æ£€æŸ¥GPUä¿¡æ¯
rocm-smi --showproductname --showuse --showtemp --showmeminfo vram

# è®¾ç½®GPUæ€§èƒ½æ¨¡å¼
sudo rocm-smi --setperflevel high

# ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
watch -n 1 rocm-smi
```

### ğŸš€ Ollamaæ€§èƒ½è°ƒä¼˜

#### ç¯å¢ƒå˜é‡ä¼˜åŒ–
```bash
# åœ¨ /etc/systemd/system/ollama.service.d/performance.conf ä¸­è®¾ç½®
Environment="OLLAMA_NUM_THREAD=16"          # åˆ©ç”¨16æ ¸æ€§èƒ½
Environment="OLLAMA_NUM_BATCH=512"          # æ‰¹å¤„ç†ä¼˜åŒ–
Environment="OLLAMA_MAX_LOADED_MODELS=4"     # é¢„åŠ è½½æ¨¡å‹
Environment="OLLAMA_KEEP_ALIVE=5m"          # ä¿æŒæ¨¡å‹æ´»è·ƒ
Environment="OLLAMA_FLASH_ATTENTION=1"       # å¯ç”¨Flash Attention
Environment="OLLAMA_F16=1"                  # ä½¿ç”¨åŠç²¾åº¦
```

#### Modelfileä¼˜åŒ–å‚æ•°
```yaml
# AMDä¼˜åŒ–å‚æ•°
PARAMETER num_thread 16          # å¤šçº¿ç¨‹å¹¶è¡Œ
PARAMETER num_batch 512          # æ‰¹å¤„ç†å¤§å°
PARAMETER f16_kv true            # åŠç²¾åº¦KVç¼“å­˜
PARAMETER use_mmap true           # å†…å­˜æ˜ å°„
PARAMETER mlock true              # é”å®šå†…å­˜
PARAMETER num_ctx 2048           # ä¸Šä¸‹æ–‡é•¿åº¦
PARAMETER num_predict 512         # ç”Ÿæˆé•¿åº¦
```

### ğŸ“Š æ€§èƒ½ç›‘æ§

#### ç³»ç»Ÿç›‘æ§
```bash
# CPUä½¿ç”¨ç‡
mpstat 1 5

# å†…å­˜ä½¿ç”¨
free -h

# ç£ç›˜IO
iostat -xz 1

# ç½‘ç»œçŠ¶æ€
sar -n DEV 1
```

#### Ollamaç›‘æ§
```bash
# OllamaæœåŠ¡çŠ¶æ€
sudo systemctl status ollama

# æŸ¥çœ‹Ollamaæ—¥å¿—
sudo journalctl -u ollama -f

# æ¨¡å‹åŠ è½½æƒ…å†µ
ollama ps
ollama list
```

#### åº”ç”¨ç›‘æ§
```bash
# Pythonè¿›ç¨‹å†…å­˜ä½¿ç”¨
ps aux | grep python

# ç«¯å£å ç”¨
netstat -tulpn | grep :8000

# å“åº”æ—¶é—´æµ‹è¯•
time curl -X POST "http://localhost:8000/chat" -H "Content-Type: application/json" -d '{"user_id":1,"content":"test"}'
```

### âš¡ é¢„æœŸæ€§èƒ½æŒ‡æ ‡

#### å“åº”æ—¶é—´
- **MetaAgentè·¯ç”±**: 200-500ms
- **EmotionAgentæƒ…æ„Ÿåˆ†æ**: 500-1000ms
- **EduAgentæ•™è‚²é—®ç­”**: 1000-2000ms
- **å®Œæ•´èŠå¤©æµç¨‹**: 1500-3000ms

#### ååé‡
- **å¹¶å‘ç”¨æˆ·**: 10-20ä¸ª
- **QPS**: 5-10è¯·æ±‚/ç§’
- **å†…å­˜ä½¿ç”¨**: 4-8GB
- **CPUä½¿ç”¨**: 30-60%

#### èµ„æºå ç”¨
- **OllamaæœåŠ¡**: 2-4GBå†…å­˜
- **Pythonåç«¯**: 1-2GBå†…å­˜
- **æ¨¡å‹åŠ è½½**: 1-2GBå†…å­˜
- **ç³»ç»Ÿé¢„ç•™**: 2-4GBå†…å­˜

## æ•…éšœæ’é™¤

### 1. OllamaæœåŠ¡æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -ano | findstr :11434

# æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
# ç¡®ä¿ç«¯å£11434æœªè¢«é˜»æ­¢
```

### 2. æ¨¡å‹åˆ›å»ºå¤±è´¥
```bash
# æ£€æŸ¥Modelfileè·¯å¾„
# ç¡®ä¿emotion_lora.ggufæ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®

# é‡æ–°åˆ›å»ºæ¨¡å‹
ollama rm emotion_lora
ollama create emotion_lora -f Modelfile_final.emotion
```

### 3. APIè°ƒç”¨å¤±è´¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags

# æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
ollama list

# æŸ¥çœ‹æ—¥å¿—
journalctl -u ollama -f  # Linuxç³»ç»Ÿ
```

### 4. æ€§èƒ½é—®é¢˜
```bash
# ç›‘æ§èµ„æºä½¿ç”¨
tasklist | findstr ollama  # Windows
top | grep ollama          # Linux

# è°ƒæ•´å‚æ•°
ollama run emotion_lora --num-ctx 1024
```

## å›æ»šåˆ°äº‘API

å¦‚æœéœ€è¦ä¸´æ—¶åˆ‡æ¢å›äº‘APIï¼Œä¿®æ”¹ `.env` æ–‡ä»¶ï¼š
```env
USE_OLLAMA=false
```

æˆ–è€…ç›´æ¥ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼š
```bash
export USE_OLLAMA=false
```

## ç›‘æ§å’Œæ—¥å¿—

### 1. Ollamaæ—¥å¿—
```bash
# æŸ¥çœ‹OllamaæœåŠ¡æ—¥å¿—
ollama ps
```

### 2. åº”ç”¨æ—¥å¿—
æŸ¥çœ‹ `backend/utils/ollama_client.py` ä¸­çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡ºã€‚

### 3. æ€§èƒ½ç›‘æ§
```bash
# æµ‹è¯•å“åº”æ—¶é—´
time curl -X POST "http://localhost:11434/api/generate" \
  -H "Content-Type: application/json" \
  -d '{"model": "emotion_lora", "prompt": "ä½ å¥½"}'
```

## æ€»ç»“

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œæ‚¨å·²æˆåŠŸå°†Happy Partnerç³»ç»Ÿä»DeepSeek APIåˆ‡æ¢åˆ°æœ¬åœ°Ollamaéƒ¨ç½²çš„emotion_loraæ¨¡å‹ã€‚è¿™æ ·åšçš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š

1. **æ•°æ®éšç§**: æ‰€æœ‰å¤„ç†éƒ½åœ¨æœ¬åœ°å®Œæˆ
2. **æˆæœ¬æ§åˆ¶**: æ— APIè°ƒç”¨è´¹ç”¨
3. **å“åº”é€Ÿåº¦**: æœ¬åœ°å¤„ç†å»¶è¿Ÿæ›´ä½
4. **å®šåˆ¶åŒ–**: å¯æ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°

å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–æˆ–æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚