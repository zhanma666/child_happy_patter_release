# æ‰‹åŠ¨æ³¨å†Œ dual_ar æ¶æ„
from transformers import AutoConfig, PretrainedConfig
import json
import torch
import logging
from typing import Optional, Dict, Any
import os

logger = logging.getLogger(__name__)

class DualARConfig(PretrainedConfig):
    model_type = "dual_ar"
    
    def __init__(
        self,
        vocab_size=10000,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072,
        hidden_act="gelu",
        hidden_dropout_prob=0.1,
        attention_probs_dropout_prob=0.1,
        max_position_embeddings=512,
        initializer_range=0.02,
        layer_norm_eps=1e-12,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.max_position_embeddings = max_position_embeddings
        self.initializer_range = initializer_range
        self.layer_norm_eps = layer_norm_eps

# æ³¨å†Œé…ç½®ç±»
AutoConfig.register("dual_ar", DualARConfig)

# åŠ è½½å®é™…é…ç½®
try:
    model_path = "/mnt/backend/models/fish-speech/checkpoints/openaudio-s1-mini"
    config_path = os.path.join(model_path, "config.json")
    
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            actual_config = json.load(f)
        
        logger.info("âœ… dual_ar æ¶æ„å·²æ‰‹åŠ¨æ³¨å†Œå¹¶åŠ è½½å®é™…é…ç½®")
    else:
        logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        
except Exception as e:
    logger.warning(f"âš ï¸ æ— æ³•åŠ è½½æ¨¡å‹é…ç½®: {e}")

class FishSpeechService:
    def __init__(self, model_name: str = "/mnt/backend/models/fish-speech/checkpoints/openaudio-s1-mini"):
        """
        åˆå§‹åŒ– Fish-Speech æœåŠ¡ã€‚
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self._load_model()

    def _load_model(self):
        """
        åŠ è½½ Fish-Speech æ¨¡å‹ã€‚
        """
        try:
            logger.info(f"Loading Fish-Speech model: {self.model_name} on device: {self.device}")
            
            # æ–¹æ³•1: é¦–å…ˆå°è¯•ä½¿ç”¨ fish_speech åŸç”ŸAPI
            self._try_fish_speech_api()
            
            # å¦‚æœåŸç”ŸAPIå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not self.model:
                self._try_alternative_methods()
                
        except Exception as e:
            logger.error(f"âŒ Failed to load Fish-Speech model: {e}")

    def _try_fish_speech_api(self):
        """
        å°è¯•ä½¿ç”¨ fish_speech åŒ…çš„åŸç”ŸAPIã€‚
        """
        try:
            from fish_speech.utils import load_model
            
            logger.info("ğŸŸ Trying to load using fish_speech native API...")
            
            # ä½¿ç”¨ fish_speech çš„åŠ è½½å‡½æ•°
            self.model = load_model(self.model_name, device=self.device)
            logger.info("âœ… Fish-Speech model loaded using native API")
            
        except ImportError:
            logger.error("âŒ fish_speech package not installed, trying alternative methods...")
            return False
        except Exception as e:
            logger.error(f"âŒ Native API failed: {e}, trying alternative methods...")
            return False
        return True

    def _try_alternative_methods(self):
        """
        å°è¯•å…¶ä»–åŠ è½½æ–¹æ³•ã€‚
        """
        try:
            # æ–¹æ³•2: ä½¿ç”¨ transformers çš„ä½çº§API
            from transformers import AutoModel
            
            logger.info("ğŸ”„ Trying to load with transformers AutoModel...")
            
            # åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(self.model_name)
            
            # åŠ è½½æ¨¡å‹ï¼Œä¸ä½¿ç”¨ device_map
            self.model = AutoModel.from_pretrained(
                self.model_name,
                config=config,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )
            
            # æ‰‹åŠ¨ç§»åŠ¨è®¾å¤‡
            self.model = self.model.to(self.device)
            logger.info("âœ… Fish-Speech model loaded using transformers")
            
        except Exception as e:
            logger.error(f"âŒ Transformers loading failed: {e}")
            
            # æ–¹æ³•3: ä½¿ç”¨æœ€åŸºç¡€çš„åŠ è½½æ–¹å¼
            try:
                logger.info("ğŸ”„ Trying basic model loading...")
                
                # ç›´æ¥ä½¿ç”¨ PyTorch åŠ è½½ï¼ˆå¦‚æœæ¨¡å‹æ˜¯ .pth æˆ– .bin æ ¼å¼ï¼‰
                model_path = self.model_name
                model_files = [f for f in os.listdir(model_path) if f.endswith(('.pth', '.bin', '.safetensors'))]
                
                if model_files:
                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹ç»“æ„æ¥åˆ›å»ºæ¨¡å‹å®ä¾‹
                    # ç”±äºæ˜¯å¤æ‚æ¨¡å‹ï¼Œæˆ‘ä»¬åªèƒ½åŠ è½½æƒé‡
                    logger.warning("âš ï¸ Basic loading: only weights can be loaded, model architecture required")
                else:
                    logger.error("âŒ No model weight files found")
                    
            except Exception as inner_e:
                logger.error(f"âŒ Basic loading also failed: {inner_e}")

    def synthesize_speech(self, text: str) -> Optional[Dict[str, Any]]:
        """
        åˆæˆè¯­éŸ³ã€‚
        """
        if not self.model:
            logger.error("âŒ Model is not loaded.")
            return None

        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„ç”Ÿæˆæ–¹æ³•
            if hasattr(self.model, 'generate'):
                # å¦‚æœæ¨¡å‹æœ‰ generate æ–¹æ³•
                result = self.model.generate(text=text)
                return {"audio": result, "sampling_rate": 24000}
            elif hasattr(self.model, 'infer'):
                # å¦‚æœæ¨¡å‹æœ‰ infer æ–¹æ³•
                result = self.model.infer(text=text)
                return {"audio": result, "sampling_rate": 24000}
            else:
                # å°è¯•è°ƒç”¨æ¨¡å‹
                logger.warning("âš ï¸ Using direct model call - may not work correctly")
                inputs = self._prepare_inputs(text)
                with torch.no_grad():
                    output = self.model(**inputs)
                return self._process_output(output)
                
        except Exception as e:
            logger.error(f"âŒ Synthesis failed: {e}")
            return None

    def _prepare_inputs(self, text: str):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰ã€‚"""
        # è¿™é‡Œæ˜¯ä¼ªä»£ç ï¼Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
        return {"input_text": text}

    def _process_output(self, output):
        """å¤„ç†æ¨¡å‹è¾“å‡ºï¼ˆéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰ã€‚"""
        # è¿™é‡Œæ˜¯ä¼ªä»£ç ï¼Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
        return {"audio": output, "sampling_rate": 24000}

    def is_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½ã€‚"""
        return self.model is not None

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•é…ç½®æ³¨å†Œ
    try:
        config = AutoConfig.from_pretrained("/mnt/backend/models/fish-speech/checkpoints/openaudio-s1-mini")
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config.model_type}")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
    
    # æµ‹è¯•æœåŠ¡åˆå§‹åŒ–
    print("\nğŸ”§ æµ‹è¯•æ¨¡å‹åŠ è½½...")
    service = FishSpeechService()
    print(f"æ¨¡å‹åŠ è½½çŠ¶æ€: {service.is_loaded()}")